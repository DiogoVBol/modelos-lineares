---
title: "Atividade 6 - Modelos Lineares"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

### Alunos: Davi Augusto, Diogo Bolzan, João Arend, Luan Frederico e Thiago Schedler.


## 1

## 2

## 3

### a) Estime e interprete os coeficientes do modelo de regressao.

```{r}
n <- 10
soma_y <- 20
soma_x1 <- 30
soma_x2 <- 40
soma_y2 <- 88.2
soma_x1_squared <- 92
soma_x2_squared <- 163
soma_yx1 <- 59
soma_yx2 <- 88
soma_x1x2 <- 119

# Matriz X'X
XTX <- matrix(c(
  n, soma_x1, soma_x2,
  soma_x1, soma_x1_squared, soma_x1x2,
  soma_x2, soma_x1x2, soma_x2_squared
), nrow = 3, byrow = TRUE)

# Vetor X'Y
XTY <- c(soma_y, soma_yx1, soma_yx2)

beta <- solve(XTX) %*% XTY
beta
# Beta_{0} = -13
# Beta_{1} = 1
# Beta_{2} = 3
```
Interpretação:

**Intercepto ($B_{0})$**: Representa o valor esperado de $Y$ (-13) quando $X_{1}$ e $X_{2}$ é zero.

**Coeficiente de $X_{1}$ ($B_{1}$)**: Indica a variação em $Y$ para cada unidade de aumento em $X_{1}$, mantendo $X_{2}$ constante.

**Intercepto ($B_{0})$**: Representa o valor esperado de $Y$ (-13) quando $X_{1}$ e $X_{2}$ é zero.

**Coeficiente de $X_{2}$ ($B_{2}$)**: Indica a variação em $Y$ para cada unidade de aumento em $X_{2}$, mantendo $X_{1}$ constante.

### b) Calcule e interprete o coeficiente de determinação ajustado

$$
R_{ajustado}^{2}=1-\frac{(1-R^{2})(n-1)}{n-k-1}
$$
Onde:

- $R^{2}$: Coeficiente de determinação comum.

- $n$: Número de observações.

- $k$: Número de variáveis independentes.

````{r}
media_y <- soma_y / n
X <- matrix(c(rep(1, n), soma_x1 / n, soma_x2 / n), ncol = 3, byrow = TRUE)
Y_hat <- X %*% beta
# Calcular o R primeiro

## Soma de quadrados total
SQtotal <- soma_y2 - n * (media_y^2)

## Soma de quadrados dos resíduos
SQres <- soma_y2 - sum(Y_hat^2)

## Soma de quadrados da regressão
SQreg <- SQtotal - SQres

# Coeficiente de determinação (R^2)
R2 <- SQreg / SQtotal

# Coeficiente de determinação ajustado (R^2 ajustado)
R2_ajustado <- 1 - ((1 - R2) * (n - 1) / (n - 3))

R2_ajustado
````
Interpretação:

- $R^{2}:$ Mede a proporçaõ de variação total de $Y$ explicada pelas variáveis $X_{1}$ e $X_{2}$.
- $R_{ajustado}^{2}:$ Ajuda o $R^{2}$ para o número de variáveis independentes do modelo, penalizando a inclusão de variáveis irrelevantes.

### c) Calcule e interprete o intervalode 95% de confoança para o coeficiente angular de X1.

````{R}
# Soma de quadrados dos resíduos
SQres <- soma_y2 - t(beta) %*% XTY

sigma2 <- as.numeric(SQres / (n - 3))

# Erro padrão do coeficiente de X1
SE_beta1 <- sqrt(sigma2 * solve(XTX)[2, 2])

t_critico <- qt(0.975, df = n - 3)

# Intervalo de confiança para o coeficiente de X1
beta1 <- beta[2]
IC_beta1 <- c(
  beta1 - t_critico * SE_beta1,
  beta1 + t_critico * SE_beta1
)

IC_beta1
````
O intervalo de confiança para o coeficiente $X_{1}$ é de [-2.47,4.47] e indica o intervalo dentro do qual o real coeficiente está presente com 95% de certeza.
O intervalo contém o zero, e portanto o coeficiente não é estatisticamente significativo, inficando que $X_{1}$ não tem um efeito relevante sobre o $Y$.

### d) Teste a hipotese de que X2 exerce influencia linear sobre Y, a 5% de nível de significancia.

````{r}
sigma2 <- as.numeric(SQres / (n - 3))

# Erro padrão do coeficiente de X2
SE_beta2 <- sqrt(sigma2 * solve(XTX)[3, 3])

# Estatística t para X2
beta2 <- beta[3]
t_stat <- beta2 / SE_beta2

# Valor crítico de t para 95% de confiança (nível de significância 5%)
t_critico <- qt(0.975, df = n - 3)

p_valor <- 2 * (1 - pt(abs(t_stat), df = n - 3))

list(
  Estatistica_t = t_stat,
  Valor_critico = t_critico,
  p_valor = p_valor
)

````
Interpretação:

Como o valor de p é menor que 0.05, há evidencias para concluir que podemos rejeitar $H_{0}$ indicando que $X_{2}$ tem influência linear significativa.

### e) Teste a significancia global da regressao, a 5% de nível de significancia. Especifique as hipóteses e interprete.

````{R}
k <- 2
df_reg <- 2
df_res <- n - 3

# Estatística F
F_stat <- (SQreg / df_reg) / (SQres / df_res)

# Valor crítico de F
F_critico <- qf(0.95, df1 = df_reg, df2 = df_res)

p_valor <- 1 - pf(F_stat, df1 = df_reg, df2 = df_res)

list(
  Estatistica_F = F_stat,
  Valor_critico = F_critico,
  p_valor = p_valor
)

````

Interpretação:

Como o valor de p é menor que 0.05, há evidencias para rejeitar $H_{0}$, indicando que o modelo é significativo.

## 4
Dada a amostra abaixo, calcule.
Y X1 X2
5 1 1
6 2 1
7 3 2
8 4 2
8 5 2
```{r, include=FALSE}

y  = c(5, 6, 7, 8, 8)
x1 = c(1, 2, 3, 4, 5)
x2 = c(1, 1, 2, 2, 2)

dados <- data.frame(
  Y  = y,
  X1 = x1,
  X2 = x2)
```
### a: Construa um intervalo de 90% de confiança para os parâmetros do modelo.
```{r 4a}

modelo <- lm(Y ~ X1 + X2, data = dados)

cat("Intervalo de confiança de 90% para os parâmetros:\n")
confint(modelo, level = 0.90)
```
### b: Qual a equação de regressão estimada?
```{r 4b}

coeficientes <- coef(modelo)

cat("Equação de regressão estimada:\n")
cat(sprintf("Y = %.2f + %.2f*X1 + %.2f*X2\n", coeficientes[1], coeficientes[2], coeficientes[3]))
```
### c: Construa a tabela ANOVA, qual a sua conclusão?
```{r 4c}

cat("Tabela anova:\n")
anova(modelo)
```
### d: Calcule e interprete os coeficientes de determinação parciais de Y X1.X2 e Y X2.X21
```{r 4d}
library(ppcor)
ry1_2 = pcor.test(y, x1, x2)
ry1_2
(ry1_2$estimate)^2

ry2_1 = pcor.test(y, x2, x1)
ry2_1

(ry2_1$estimate)^2
```
Portanto `r (ry1_2$estimate)^2 * 100`% da variabilidade de Y é explicada por X1 mantendo X2 constante e `r (ry2_1$estimate)^2 * 100`% da variabilidade de Y é explicada por X2 mantendo X1 constante.
**obs:** Os valores estimados para coeficiente de correlação parcial não foram significativos.

## 5

## 6